import os
import numpy as np
import argparse
import csv
from sklearn import metrics
from sklearn.naive_bayes import BernoulliNB
from sklearn.svm import SVC
from sklearn.linear_model import Perceptron
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import roc_auc_score
from sklearn.metrics import accuracy_score
from sklearn.externals import joblib

#exract the absolute path for a filename and push it to a list
#@return: list of filenames (in abslute path)
def absoluteFilePaths(directory):
    res = []
    for dirpath,_,filenames in os.walk(directory):
       for f in filenames:
           res+= [os.path.abspath(os.path.join(dirpath, f))]
    return res



#this function read the dataset and push the filenames in two lists, one for malware and one for bening app
#@return: (list of bening path/:filenames, list of malware path/:filenames)
def read_data(dataset_dir = "../drebin"):
    # list that contains the list of all the files contained in dataset_dir
    files_list = absoluteFilePaths(dataset_dir+"/feature_vectors")
    benign_list = []
    malware_list = []
    with open(dataset_dir+'/sha256_family.csv') as csv_file:
        csv_reader = csv.reader(csv_file, delimiter=',')
        line_count = 0
        for row in csv_reader:
            if line_count == 0:
                print("reading csv for Bad guys")
                line_count += 1
            else:
                malware_list+= [dataset_dir +"/feature_vectors/"+ row[0]]
                line_count += 1
        print(f'Processed {line_count} lines.')

    benign_list = [good for good in files_list if good not in malware_list]
    return benign_list,malware_list




#this function take as parameters two list of elements of the form path/:filename, one for bening app, one for malware and transform the
#A corpus of documents can thus be represented by a matrix with one row per document and one column per token (e.g. word) occurring in the corpus.
#We call vectorization the general process of turning a collection of text documents into numerical feature vectors. This specific strategy
#(tokenization, counting and normalization) is called the Bag of Words or “Bag of n-grams” representation. Documents are described by word
#occurrences while completely ignoring the relative position information of the words in the document.
# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html

# NOTE in the following the order will be the same : benign_file_list + malign_file_list
#@params: list of bening path/:filenames, list of malware path/:filenames
#@return: (sparse matrix for the feature extracted (hot encoder), sparse matrix for the labels)
def create_feature_vector(benign_list,malware_list):
    # we have bening + malign , so we then create the trainig set X
    vectorizer = TfidfVectorizer(input="filename", tokenizer=lambda x: x.split('\n'), token_pattern=None, binary=True)
    X = vectorizer.fit_transform( benign_list + malware_list )
    #TODO fit vs fit transform ?
    # now the target labels, since the lists are in the same domain we just create 2 vector (one for each domain) and then we merge them
    y_malign = np.ones(len(malware_list))
    y_benign = np.empty(len(benign_list))
    y_benign.fill(-1)
    Y = np.concatenate( ( y_benign , y_malign ) , axis=None)
    return X,Y




# this method is just a wrapper for a classifier that fit the data in input and perform the test
# then it computes some evaluation metrics about the classifier
# @params: the classifier instance , the training and test datasets
# @return: the trained classifier and the accuracy w.r.t. the test set
def classifier_fit_predict_score( label, classifier , X_train, X_test, y_train, y_test):
    classifier.fit(X_train , y_train)
    y_predict = classifier.predict(X_test)
    accuracy = accuracy_score(y_test , y_predict)
    with open("./detection/"+label+"_output.txt", "a+") as text_file:
        print (f'ROC area under curve for {label} classifier: ',roc_auc_score( y_test, y_predict ) , file=text_file)
        print (metrics.classification_report(y_test,y_predict, labels=[1, -1],target_names=['Malware', 'BenignApp']) , file=text_file)
    return classifier,accuracy






def main(Args):
    b, m = read_data(Args.dataset)
    X,y = create_feature_vector(b,m)
    # K-Folds cross-validator
    # Split dataset into k consecutive folds.
    # in this experiment we will use the k-fold cross-valudation technique to decrease the effect of the bias that may arise with not including
    # all the dataset in the learning phase. also we will use the Stratified Kfold from sklearn library that
    # Provides train/test indices to split data in train/test sets. This cross-validation object is a variation of KFold that returns
    # stratified folds. The folds are made by preserving the percentage of samples for each class.
    skf = StratifiedKFold(n_splits=Args.cv,  shuffle=True , random_state=5 )
    classifiers_list = {}
    if not os.path.exists("./detection"):
        os.makedirs("./detection")
    classifiers_list["bernoulli"] = []
    classifiers_list["svm"] = []
    classifiers_list["perceptron"] = []
    for train_index, test_index in skf.split(X, y):
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]

        bernoulli_clf = BernoulliNB()
        clf , accuracy = classifier_fit_predict_score( "Bernoulli", bernoulli_clf, X_train, X_test, y_train, y_test)
        classifiers_list["bernoulli"] += [(clf,accuracy)]

        svm_clf = SVC(kernel=Args.SVMkernel)
        clf , accuracy = classifier_fit_predict_score( "SVM", svm_clf, X_train, X_test, y_train, y_test)
        classifiers_list["svm"] += [(clf,accuracy)]

        perceptron_clf = Perceptron(eta0 = 0.1, max_iter=100, tol=1e-3)
        clf , accuracy = classifier_fit_predict_score( "Perceptron", perceptron_clf, X_train, X_test, y_train, y_test)
        classifiers_list["perceptron"] += [(clf,accuracy)]

    print( "classification for malware detection complete see output scores in the txt files in ./detection" )
    print (f'we select the best estimator among the {Args.cv} for each of the classifiers and save it in a file as classifier.joblib'  )
    for clf_name in classifiers_list.keys():
        best_estimator , best_accuracy = classifiers_list[clf_name][0]
        for estimator,accuracy in classifiers_list[clf_name]:
            if accuracy > best_accuracy:
                best_accuracy = accuracy
                best_estimator = estimator
        joblib.dump(best_estimator, "./detection/"+clf_name+'.joblib')
        print (f'Best estimator for {clf_name} has accuracy of {best_accuracy}')









def ParseArgs():
    Args = argparse.ArgumentParser(description="description: this script will train 3 classifiers namely naive-Bernoulli, svm , and perceptron for the task of malware detection" )
    Args.add_argument("--SVMkernel", default= "linear",help="the kernel for the svm classifier")
    Args.add_argument("--cv", type=int,default=3,help="number of set to split the dataset, default 3")
    Args.add_argument("--dataset",default="../drebin",help="the path to the dataset directory")
    return Args.parse_args()





if __name__ == "__main__":
    main(ParseArgs())
