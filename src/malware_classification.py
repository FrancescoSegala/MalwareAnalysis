import os
import numpy as np
import argparse
import csv
from sklearn import metrics
from sklearn.naive_bayes import BernoulliNB
from sklearn.svm import SVC
from sklearn.linear_model import Perceptron
from sklearn.multiclass import OneVsRestClassifier
from sklearn.multiclass import OneVsOneClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import roc_auc_score
from sklearn.metrics import accuracy_score
from sklearn.externals import joblib
import warnings


# NOTE  All classifiers in scikit-learn do multiclass classification out-of-the-box.
# All scikit-learn classifiers are capable of multiclass classification, but the meta-estimators offered by sklearn.multiclass permit
# changing the way they handle more than two classes because this may have an effect on classifier performance (either in terms of generalization error
# or required computational resources).

# we deliberately ignore the warnings because can happen that in the prediction set there are less classes that in the training set
# so it would generate a warning, our choice is to keep the labels in the metric result with a F1-score 0.0 and not ignoring them
warnings.filterwarnings('ignore')


# this method should read the dataset and present a dictionary : (k,v) = (family,[malware_x, malware_y,...,malware_n])
# We also filter the malware family that has 20+ instances in the dataset
def read_dataset(dataset_dir = "../drebin", family_size = 20):
    malware_family = {}
    with open(dataset_dir+'/sha256_family.csv') as csv_file:
        csv_reader = csv.reader(csv_file, delimiter=',')
        line_count = 0
        for row in csv_reader:
            if line_count == 0:
                print("reading dataset...")
                line_count += 1
            else:
                if row[1] in malware_family.keys():
                    malware_family[row[1]]+=[os.path.abspath(dataset_dir+"/feature_vectors/"+row[0])]
                else :
                    malware_family[row[1]] = [os.path.abspath(dataset_dir+"/feature_vectors/"+row[0])]
                line_count += 1
        # filter families that has too few instances in D
        aux = {}
        print ("number of families before pruning ",len(malware_family.keys()))
        for key in malware_family.keys():
            if len(malware_family[key]) >= family_size :
                aux[key] = malware_family[key]
        print ("number of families after pruning ",len(aux.keys()))
        return aux




# this function transform the corpus of documents that can thus be represented by a matrix with one row per document and one column per token
# (e.g. word) occurring in the corpus. We call vectorization the general process of turning a collection of text documents into numerical feature vectors.
# This specific strategy (tokenization, counting and normalization) is called the Bag of Words or “Bag of n-grams” representation.
#Documents are described by word occurrences while completely ignoring the relative position information of the words in the document.
# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html

#@params: python dict with (k,v) = (family, [path:/malware_1,path:/malware_2,...,path:/malware_n])
#@return: (sparse matrix for the feature extracted (hot encoder), sparse matrix for the labels)
def create_feature_vector(malware_family):
    family = 0
    malwares = []
    y = []
    for key in malware_family.keys():
        for malware in malware_family[key]:
            malwares+=[malware]
            y += [family]
        family += 1
    vectorizer = TfidfVectorizer(input="filename", tokenizer=lambda x: x.split('\n'), token_pattern=None, binary=True)
    X = vectorizer.fit_transform( malwares )
    return X,np.asarray(y)



# this method is just a wrapper for a classifier that fit the data in input and perform the test
# then it computes some evaluation metrics about the classifier
# @params: the classifier instance , the training and test datasets , the family labels for computing the score
# @return: the trained classifier and the accuracy w.r.t. the test set
def classifier_fit_predict_score( label, classifier , X_train, X_test, y_train, y_test, classes):
    classifier.fit(X_train , y_train)
    y_predict = classifier.predict(X_test)
    accuracy = accuracy_score(y_test , y_predict)
    with open("./classification/"+label+"_output.txt", "a+") as text_file:
        print ("accuracy :", accuracy, file = text_file)
        print (metrics.classification_report(y_test,y_predict,target_names=classes) , file=text_file)
    return classifier,accuracy



# this method is just a wrapper for a classifier that fit the data in input and perform the test
# then it computes some evaluation metrics about the classifier
# @params: the classifier instance , the training and test datasets , the family labels for computing the score
# @return: the trained classifier and the accuracy w.r.t. the test set
def classifier_fit_predict_score_multiclass_module( label, classifier , X_train, X_test, y_train, y_test, classes):
    y_predict = OneVsRestClassifier(classifier).fit(X_train , y_train).predict(X_test)
    #y_predict = OneVsOneClassifier(classifier).fit(X_train , y_train).predict(X_test) NOTE this is very costly O(n_classes ^2)
    accuracy = accuracy_score(y_test , y_predict)
    with open("./classification/"+label+"_output.txt", "a+") as text_file:
        print ("accuracy :", accuracy, file = text_file)
        print (metrics.classification_report(y_test,y_predict,target_names=classes) , file=text_file)
    return classifier,accuracy



def main(Args):

    families = read_dataset(Args.dataset, Args.familySize)
    X, y = create_feature_vector(families)

    classifiers_list = {}
    if not os.path.exists("./classification"):
        os.makedirs("./classification")
    classifiers_list["bernoulli"] = []
    classifiers_list["svm"] = []
    classifiers_list["perceptron"] = []

    skf = StratifiedKFold(n_splits=Args.cv,  shuffle=True , random_state=5 )

    print(f'multiclass classifier from sci-kit is set to {Args.multiclass}')
    for train_index, test_index in skf.split(X, y):
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]

        if Args.multiclass:
                bernoulli_clf = BernoulliNB()
                clf , accuracy = classifier_fit_predict_score_multiclass_module( "Bernoulli", bernoulli_clf, X_train, X_test, y_train, y_test, families.keys())
                classifiers_list["bernoulli"] += [(clf,accuracy)]
        else :
            bernoulli_clf = BernoulliNB()
            clf , accuracy = classifier_fit_predict_score( "Bernoulli", bernoulli_clf, X_train, X_test, y_train, y_test, families.keys())
            classifiers_list["bernoulli"] += [(clf,accuracy)]

        if Args.multiclass:
            svm_clf = SVC(kernel=Args.SVMkernel)
            clf , accuracy = classifier_fit_predict_score_multiclass_module( "SVM", svm_clf, X_train, X_test, y_train, y_test, families.keys())
            classifiers_list["svm"] += [(clf,accuracy)]
        else:
            svm_clf = SVC(kernel=Args.SVMkernel)
            clf , accuracy = classifier_fit_predict_score( "SVM", svm_clf, X_train, X_test, y_train, y_test, families.keys())
            classifiers_list["svm"] += [(clf,accuracy)]
        if Args.multiclass:
            perceptron_clf = Perceptron(eta0 = 0.1, max_iter=100, tol=1e-3)
            clf , accuracy = classifier_fit_predict_score_multiclass_module( "Perceptron", perceptron_clf, X_train, X_test, y_train, y_test, families.keys())
            classifiers_list["perceptron"] += [(clf,accuracy)]
        else:
            perceptron_clf = Perceptron(eta0 = 0.1, max_iter=100, tol=1e-3)
            clf , accuracy = classifier_fit_predict_score( "Perceptron", perceptron_clf, X_train, X_test, y_train, y_test, families.keys())
            classifiers_list["perceptron"] += [(clf,accuracy)]


    print( "classification for malware family complete see output scores in the txt files in ./classification" )
    print (f'we select the best estimator among the {Args.cv} for each of the classifiers and save it in a file as classifier.joblib'  )
    for clf_name in classifiers_list.keys():
        best_estimator , best_accuracy = classifiers_list[clf_name][0]
        for estimator,accuracy in classifiers_list[clf_name]:
            if accuracy > best_accuracy:
                best_accuracy = accuracy
                best_estimator = estimator
        joblib.dump(best_estimator, "./classification/"+clf_name+'.joblib')
        print (f'Best estimator for {clf_name} has accuracy of {best_accuracy}')





def ParseArgs():
    Args = argparse.ArgumentParser(description="description: this script will train 3 classifiers namely naive-Bernoulli, svm , and perceptron for the task of malware classification" )
    Args.add_argument("--SVMkernel", default= "linear",help="the kernel for the svm classifier")
    Args.add_argument("--cv", type=int,default=3,help="number of set to split the dataset, default 3")
    Args.add_argument("--dataset",default="../drebin",help="the path to the dataset directory")
    Args.add_argument("--multiclass",default=False,help="enabling this you allow to use the multiclass learning from sklearn, it could affect the performances", action="store_true")
    Args.add_argument("--familySize",type=int,default=20,help="the size from wich a family is taken into account")
    return Args.parse_args()





if __name__ == "__main__":
    main(ParseArgs())

#
